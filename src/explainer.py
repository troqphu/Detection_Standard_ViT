import numpy as np
from typing import Dict, List, Tuple, Optional
import warnings
import cv2
from PIL import Image
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from scipy import ndimage
from matplotlib.colors import LinearSegmentedColormap
import matplotlib.pyplot as plt

# Set CV2_AVAILABLE since we've imported cv2 successfully
CV2_AVAILABLE = True

# üî• Import product-specific knowledge
try:
    from product_knowledge import ProductAnalyzer
    PRODUCT_KNOWLEDGE_AVAILABLE = True
    print("‚úÖ ProductAnalyzer loaded successfully")
except ImportError as e:
    PRODUCT_KNOWLEDGE_AVAILABLE = False
    print(f"‚ö†Ô∏è Product knowledge module not available: {e}")

try:
    import yaml
    import timm
    from PIL import Image
    from torchvision import transforms
    from datetime import datetime
except ImportError as e:
    print(f"‚ö†Ô∏è Some optional modules not available: {e}")

warnings.filterwarnings('ignore')

class ExplainabilityAnalyzer:
    def get_colored_heatmap_overlay(self, original_image: np.ndarray, heatmap: np.ndarray, alpha: float = 0.4) -> np.ndarray:
        """Tr·∫£ v·ªÅ ·∫£nh overlay heatmap m√†u (OpenCV) cho web/API. ƒê·∫£m b·∫£o lu√¥n l√† ·∫£nh m√†u."""
        import cv2
        # Normalize heatmap to [0, 1]
        hmin, hmax = np.min(heatmap), np.max(heatmap)
        if hmax - hmin > 1e-6:
            norm_heatmap = (heatmap - hmin) / (hmax - hmin)
        else:
            norm_heatmap = np.zeros_like(heatmap)
        heatmap_uint8 = (norm_heatmap * 255).astype(np.uint8)
        heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)
        # Ensure original image is 3-channel
        if original_image.ndim == 2:
            original = cv2.cvtColor(original_image, cv2.COLOR_GRAY2BGR)
        elif original_image.shape[2] == 1:
            original = cv2.cvtColor(original_image, cv2.COLOR_GRAY2BGR)
        else:
            original = original_image.copy()
        if original.max() <= 1.0:
            original = (original * 255).astype(np.uint8)
        overlay = cv2.addWeighted(original, 1 - alpha, heatmap_color, alpha, 0)
        return overlay
    """
    üî• ENHANCED Explainability Analyzer for Vision Transformers
    Features: Gradient-based attention, Multi-scale fusion, Advanced visualization
    """
    def __init__(self, model: nn.Module, class_names: List[str] = None):
        self.model = model.eval()
        self.class_names = class_names or ['Fake', 'Real']
        self.device = next(model.parameters()).device
        
        # Enhanced model detection
        self.is_timm_model = 'timm' in str(type(model)) or hasattr(model, 'default_cfg')
        
        # üî• NEW: Advanced caching system
        self._attention_cache = {}
        self._gradient_cache = {}
        
        # üî• NEW: Multiple attention extraction methods
        self.attention_maps = []
        self.activation_maps = []
        self.gradients = {}
        self.feature_maps = {}
        
        # üî• NEW: Custom colormaps for better visualization
        self._setup_custom_colormaps()
        
        # üî• NEW: Product-specific analyzer
        if PRODUCT_KNOWLEDGE_AVAILABLE:
            self.product_analyzer = ProductAnalyzer()
        else:
            self.product_analyzer = None
        
        if self.is_timm_model:
            self._setup_enhanced_hooks()

    def _setup_custom_colormaps(self):
        """Setup custom color schemes for different attention types"""
        # Heat colormap (red-yellow-white)
        colors_heat = ['#000033', '#000066', '#003399', '#0066CC', '#33AAFF', '#66CCFF', '#FFFF00', '#FF9900', '#FF3300', '#FFFFFF']
        self.cmap_heat = LinearSegmentedColormap.from_list('custom_heat', colors_heat)
        
        # Cool colormap (blue-cyan-green)
        colors_cool = ['#000066', '#0033AA', '#0066DD', '#3399FF', '#66CCFF', '#99FFFF', '#CCFFCC', '#99FF99', '#66FF66', '#33FF33']
        self.cmap_cool = LinearSegmentedColormap.from_list('custom_cool', colors_cool)
        
        # Focus colormap (purple-magenta-yellow)
        colors_focus = ['#330066', '#660099', '#9900CC', '#CC00FF', '#FF33FF', '#FF66CC', '#FF9999', '#FFCC66', '#FFFF33', '#FFFFFF']
        self.cmap_focus = LinearSegmentedColormap.from_list('custom_focus', colors_focus)

    def _setup_enhanced_hooks(self):
        """üî• ENHANCED: Multi-level hooks for comprehensive attention extraction"""
        def gradient_hook(module, grad_input, grad_output):
            """Capture gradients for gradient-based attention"""
            module_name = self._get_module_name(module)
            if grad_output[0] is not None:
                self.gradients[module_name] = grad_output[0].detach()

        def forward_hook(module, input, output):
            """Capture forward activations and attention weights"""
            module_name = self._get_module_name(module)
            try:
                # Store feature maps
                if isinstance(output, torch.Tensor):
                    self.feature_maps[module_name] = output.detach()
                    
                    # Handle different output formats
                    if output.ndim == 4:  # [B, H, N, N] - attention weights
                        self.attention_maps.append(output.detach())
                    elif output.ndim == 3:  # [B, N, D] - token embeddings
                        self.activation_maps.append(output.detach())
                        
                elif isinstance(output, (tuple, list)):
                    for i, out in enumerate(output):
                        if isinstance(out, torch.Tensor):
                            self.feature_maps[f"{module_name}_output_{i}"] = out.detach()
                            if out.ndim == 4:
                                self.attention_maps.append(out.detach())
                            elif out.ndim == 3:
                                self.activation_maps.append(out.detach())
            except Exception as e:
                pass

        # Register hooks on multiple layers for multi-scale analysis
        self.hooks = []
        hook_targets = [
            'attn.attn_drop',  # After attention dropout
            'attn',            # Attention module itself
            'blocks',          # Transformer blocks
            'norm1',           # Layer norm after attention
            'norm2',           # Layer norm after MLP
            'head',            # Classification head
        ]

        hooks_registered = 0
        for name, module in self.model.named_modules():
            for target in hook_targets:
                if target in name:
                    try:
                        # Register both forward and backward hooks
                        self.hooks.append(module.register_forward_hook(forward_hook))
                        self.hooks.append(module.register_backward_hook(gradient_hook))
                        hooks_registered += 1
                        break
                    except Exception as e:
                        continue
        
        print(f"üîó Registered {hooks_registered} enhanced hooks for attention extraction")

    def _get_module_name(self, module):
        """Get a unique name for a module"""
        for name, mod in self.model.named_modules():
            if mod is module:
                return name
        return f"unknown_module_{id(module)}"

    def _get_enhanced_attention_for_image(self, image_tensor: torch.Tensor, target_class: int = None) -> Dict[str, np.ndarray]:
        """
        üî• ENHANCED: Multi-method attention extraction for superior heatmaps
        """
        self.attention_maps = []
        self.activation_maps = []
        self.gradients = {}
        self.feature_maps = {}
        
        # Enable gradient computation
        image_tensor.requires_grad_(True)
        
        # Forward pass with gradient tracking
        logits = self.model(image_tensor.unsqueeze(0))
        
        # Use predicted class if target not specified
        if target_class is None:
            target_class = torch.argmax(logits, dim=1).item()
        
        # Backward pass for gradient-based attention
        class_score = logits[0, target_class]
        class_score.backward(retain_graph=True)
        
        # Collect multiple attention types
        attention_results = {}
        
        # 1. üî• Gradient-based attention (most accurate)
        try:
            gradient_attention = self._compute_gradient_attention(image_tensor)
            attention_results['gradient'] = gradient_attention
        except Exception as e:
            print(f"Gradient attention failed: {e}")
        
        # 2. Traditional attention weights
        try:
            weight_attention = self._process_attention_maps()
            attention_results['weights'] = weight_attention
        except Exception as e:
            print(f"Weight attention failed: {e}")
        
        # 3. Activation-based attention
        try:
            activation_attention = self._process_activation_maps()
            attention_results['activation'] = activation_attention
        except Exception as e:
            print(f"Activation attention failed: {e}")
        
        # 4. üî• Multi-scale fusion
        try:
            if len(attention_results) > 1:
                fused_attention = self._fuse_attention_maps(attention_results)
                attention_results['fused'] = fused_attention
        except Exception as e:
            print(f"Attention fusion failed: {e}")
        
        # Return best available attention or fallback
        if 'fused' in attention_results:
            primary_attention = attention_results['fused']
        elif 'gradient' in attention_results:
            primary_attention = attention_results['gradient']
        elif 'weights' in attention_results:
            primary_attention = attention_results['weights']
        elif 'activation' in attention_results:
            primary_attention = attention_results['activation']
        else:
            primary_attention = self._create_fallback_heatmap(image_tensor.shape[1:])
            attention_results['fallback'] = primary_attention
        
        # üî• Post-process for enhanced visualization
        enhanced_attention = self._enhance_heatmap(primary_attention)
        attention_results['enhanced'] = enhanced_attention
        
        return attention_results

    def _compute_gradient_attention(self, image_tensor: torch.Tensor) -> np.ndarray:
        """üî• NEW: Compute gradient-based attention using GradCAM-like approach"""
        try:
            gradients = image_tensor.grad
            if gradients is None:
                raise ValueError("No gradients available")
            
            # Get activations from the last feature map
            if not self.feature_maps:
                raise ValueError("No feature maps captured")
            
            # Use the most relevant feature map (usually from the last transformer block)
            feature_map_key = max(self.feature_maps.keys(), key=lambda x: 'blocks' in x and 'norm' in x)
            activations = self.feature_maps[feature_map_key]
            
            # Remove batch dimension and handle patch tokens
            if activations.dim() == 3:  # [B, N, D]
                activations = activations[0]  # [N, D]
                if activations.shape[0] > 196:  # Remove CLS token if present
                    activations = activations[1:]  # [196, D] for 14x14 patches
            
            # FIXED: Handle gradient dimensions properly
            if gradients.dim() == 4:  # [B, C, H, W]
                gradient_weights = torch.mean(gradients[0], dim=(1, 2))  # [C]
            elif gradients.dim() == 3:  # [B, N, D]
                gradient_weights = torch.mean(gradients[0], dim=0)  # [D]
            else:
                # Fallback: use mean pooling
                gradient_weights = torch.mean(gradients[0].flatten())
                gradient_weights = gradient_weights.repeat(activations.shape[-1])
            
            # Ensure dimensions match
            if gradient_weights.shape[0] != activations.shape[-1]:
                # Resize gradient weights to match activations
                gradient_weights = F.adaptive_avg_pool1d(
                    gradient_weights.unsqueeze(0).unsqueeze(0), 
                    activations.shape[-1]
                ).squeeze()
            
            # Weight activations by gradients
            weighted_activations = torch.sum(activations * gradient_weights.unsqueeze(0), dim=1)
            
            # Apply ReLU to keep only positive influence
            attention_weights = F.relu(weighted_activations).cpu().numpy()
            
            return self._reshape_to_heatmap(attention_weights)
            
        except Exception as e:
            print(f"Gradient attention failed: {e}")
            # Return random heatmap as fallback
            return np.random.rand(224, 224) * 0.1

    def _fuse_attention_maps(self, attention_dict: Dict[str, np.ndarray]) -> np.ndarray:
        """üî• NEW: Intelligently fuse multiple attention maps"""
        valid_maps = []
        weights = []
        
        # Priority weighting for different attention types
        priority_weights = {
            'gradient': 0.4,    # Highest priority - most accurate
            'weights': 0.3,     # High priority - direct attention
            'activation': 0.2,  # Medium priority - indirect
            'fallback': 0.1     # Lowest priority - emergency only
        }
        
        for att_type, att_map in attention_dict.items():
            if att_type in priority_weights and att_map is not None:
                # Normalize each map
                normalized_map = self._normalize_heatmap(att_map)
                valid_maps.append(normalized_map)
                weights.append(priority_weights[att_type])
        
        if not valid_maps:
            raise ValueError("No valid attention maps to fuse")
        
        # Weighted average fusion
        weights = np.array(weights) / np.sum(weights)  # Normalize weights
        fused_map = np.zeros_like(valid_maps[0])
        
        for map_data, weight in zip(valid_maps, weights):
            fused_map += weight * map_data
        
        return fused_map

    def _enhance_heatmap(self, heatmap: np.ndarray, method='advanced') -> np.ndarray:
        """üî• NEW: Advanced heatmap enhancement for better visualization"""
        if method == 'advanced':
            # Multi-step enhancement pipeline
            enhanced = heatmap.copy()
            
            # 1. Gaussian smoothing to reduce noise
            enhanced = ndimage.gaussian_filter(enhanced, sigma=1.0)
            
            # 2. Enhance contrast using histogram equalization
            enhanced_flat = enhanced.flatten()
            enhanced_eq = np.interp(enhanced_flat, 
                                   np.linspace(enhanced_flat.min(), enhanced_flat.max(), 256),
                                   np.linspace(0, 1, 256))
            enhanced = enhanced_eq.reshape(enhanced.shape)
            
            # 3. Apply bilateral filter for edge-preserving smoothing
            enhanced_8bit = (enhanced * 255).astype(np.uint8)
            enhanced = cv2.bilateralFilter(enhanced_8bit, 9, 75, 75).astype(np.float32) / 255.0
            
            # 4. Enhance high-attention regions
            threshold = np.percentile(enhanced, 75)
            mask = enhanced > threshold
            enhanced[mask] = enhanced[mask] ** 0.7  # Gamma correction for highlights
            
            return enhanced
        else:
            # Simple enhancement
            return ndimage.gaussian_filter(heatmap, sigma=0.8)

    def _normalize_heatmap(self, heatmap: np.ndarray) -> np.ndarray:
        """Normalize heatmap to [0, 1] range"""
        hmap_min, hmap_max = heatmap.min(), heatmap.max()
        if hmap_max > hmap_min:
            return (heatmap - hmap_min) / (hmap_max - hmap_min)
        else:
            return np.ones_like(heatmap) * 0.5

    def _process_attention_maps(self) -> np.ndarray:
        """Process attention weights to create heatmap."""
        processed_attentions = []
        
        for attn_map in self.attention_maps:
            try:
                # Handle different attention map shapes
                if attn_map.ndim == 4:  # [B, H, N, N]
                    # Focus on the attention from the [CLS] token to the image patches
                    if attn_map.shape[2] > 1 and attn_map.shape[3] > 1:
                        cls_attention = attn_map[0, :, 0, 1:].mean(dim=0)  # Avg over heads
                        processed_attentions.append(cls_attention.cpu().numpy())
                elif attn_map.ndim == 3:  # [B, N, N]
                    if attn_map.shape[1] > 1 and attn_map.shape[2] > 1:
                        cls_attention = attn_map[0, 0, 1:]  # CLS to patches
                        processed_attentions.append(cls_attention.cpu().numpy())
            except Exception as e:
                continue
        
        if not processed_attentions:
            raise ValueError("No valid attention maps found")

        # Average the attention maps from all transformer blocks
        final_attention = np.mean(processed_attentions, axis=0)
        return self._reshape_to_heatmap(final_attention)

    def _process_activation_maps(self) -> np.ndarray:
        """Process activation maps to create attention-like heatmap."""
        if not self.activation_maps:
            raise ValueError("No activation maps available")
        
        # Use the last activation map (closest to the output)
        last_activation = self.activation_maps[-1][0]  # Remove batch dimension
        
        # If this includes CLS token, remove it
        if last_activation.shape[0] > 196:  # Assuming 14x14 patches + CLS
            patch_activations = last_activation[1:]  # Remove CLS token
        else:
            patch_activations = last_activation
        
        # Compute attention-like weights from activations
        attention_weights = torch.norm(patch_activations, dim=1).cpu().numpy()
        return self._reshape_to_heatmap(attention_weights)

    def _reshape_to_heatmap(self, attention_weights: np.ndarray, target_size: Tuple[int, int] = (224, 224)) -> np.ndarray:
        """Reshape 1D attention weights to 2D heatmap."""
        # Determine grid size
        grid_size = int(np.sqrt(attention_weights.shape[0]))
        if grid_size * grid_size != attention_weights.shape[0]:
            # Handle non-square cases by padding or truncating
            expected_size = grid_size * grid_size
            if attention_weights.shape[0] > expected_size:
                attention_weights = attention_weights[:expected_size]
            else:
                padding = expected_size - attention_weights.shape[0]
                attention_weights = np.pad(attention_weights, (0, padding), mode='constant')

        attention_grid = attention_weights.reshape(grid_size, grid_size)
        
        # Resize to target image size
        heatmap = cv2.resize(attention_grid, target_size, interpolation=cv2.INTER_CUBIC)
        
        # Normalize
        heatmap_min, heatmap_max = heatmap.min(), heatmap.max()
        if heatmap_max > heatmap_min:
            heatmap = (heatmap - heatmap_min) / (heatmap_max - heatmap_min)
        else:
            heatmap = np.ones_like(heatmap) * 0.5
            
        return heatmap

    def _create_fallback_heatmap(self, image_shape: Tuple[int, ...]) -> np.ndarray:
        """Creates a center-biased heatmap as a fallback."""
        if len(image_shape) == 2:
            h, w = image_shape
        elif len(image_shape) == 3:
            h, w = image_shape[1], image_shape[2]
        else:
            h, w = 224, 224  # Default size
            
        y, x = np.ogrid[:h, :w]
        center_y, center_x = h / 2, w / 2
        heatmap = np.exp(-((x - center_x)**2 + (y - center_y)**2) / (min(h, w) * 0.4)**2)
        return heatmap

    def predict_with_explanation(self, image_tensor: torch.Tensor, original_image: np.ndarray) -> Dict:
        """
        üî• ENHANCED: Generate prediction with multi-faceted explanation and superior heatmaps
        """
        # Ensure correct tensor shape
        if image_tensor.dim() == 3:
            image_tensor = image_tensor.unsqueeze(0)
        
        with torch.no_grad():
            logits = self.model(image_tensor)
            probabilities = F.softmax(logits, dim=1)
            confidence, predicted_class_idx = torch.max(probabilities, dim=1)
        
        predicted_class = self.class_names[predicted_class_idx.item()]
        
        # üî• Get enhanced multi-method attention
        try:
            attention_results = self._get_enhanced_attention_for_image(
                image_tensor.squeeze(0), 
                target_class=predicted_class_idx.item()
            )
            
            # Use the best available heatmap
            if 'enhanced' in attention_results:
                primary_heatmap = attention_results['enhanced']
            elif 'fused' in attention_results:
                primary_heatmap = attention_results['fused']
            else:
                primary_heatmap = list(attention_results.values())[0]
                
        except Exception as e:
            print(f"Warning: Using fallback heatmap due to error: {e}")
            primary_heatmap = self._create_fallback_heatmap(original_image.shape[:2])
            attention_results = {'fallback': primary_heatmap}
        
        # üî• Enhanced content analysis
        content_analysis = self._analyze_image_content_enhanced(original_image, primary_heatmap)
        
        # üî• Product-specific analysis
        product_specific_data = {}
        if self.product_analyzer:
            try:
                is_fake = predicted_class.lower() == 'fake'
                product_type = self.product_analyzer.detect_product_type(original_image, content_analysis)
                product_specific_features = self.product_analyzer.analyze_product_specific_features(
                    original_image, content_analysis, product_type, is_fake
                )
                product_specific_explanation = self.product_analyzer.generate_product_specific_explanation(
                    product_type, product_specific_features, is_fake, confidence.item()
                )
                
                product_specific_data = {
                    'product_type': product_type,
                    'specific_features': product_specific_features,
                    'specific_explanation': product_specific_explanation
                }
            except Exception as e:
                print(f"Product-specific analysis failed: {e}")
        
        # SHORT VIETNAMESE EXPLANATION
        explanation_text = self._generate_vietnamese_compact(
            predicted_class, confidence.item(), content_analysis, attention_results
        )
        
        return {
            'prediction': predicted_class,
            'confidence': confidence.item(),
            'explanation': explanation_text,
            'heatmap': primary_heatmap,
            'all_heatmaps': attention_results,  # üî• NEW: Multiple heatmap types
            'content_analysis': content_analysis,
            'product_analysis': product_specific_data,  # üî• NEW: Product-specific data
            'model_type': 'timm_pretrained' if self.is_timm_model else 'custom',
            'attention_methods': list(attention_results.keys())  # üî• NEW: Available methods
        }

    def _analyze_image_content_enhanced(self, image: np.ndarray, heatmap: np.ndarray) -> Dict:
        """üî• ENHANCED: Advanced image content analysis with multiple metrics"""
        try:
            gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            h, w = gray_image.shape
            
            # Ensure heatmap matches image size
            if heatmap.shape != (h, w):
                heatmap = cv2.resize(heatmap, (w, h), interpolation=cv2.INTER_CUBIC)
            
            # üî• Multi-level attention analysis
            focus_masks = {
                'high': (heatmap > np.percentile(heatmap, 90)).astype(np.uint8),
                'medium': (heatmap > np.percentile(heatmap, 70)).astype(np.uint8),
                'low': (heatmap > np.percentile(heatmap, 50)).astype(np.uint8)
            }
            
            analysis = {}
            
            # üî• DETAILED VISUAL FEATURE ANALYSIS
            analysis.update(self._analyze_material_properties(image, focus_masks))
            analysis.update(self._analyze_manufacturing_details(image, gray_image, focus_masks))
            analysis.update(self._analyze_color_patterns(image, focus_masks))
            analysis.update(self._analyze_geometric_features(image, gray_image, focus_masks))
            analysis.update(self._analyze_surface_texture(gray_image, focus_masks))
            
            # Attention distribution analysis
            for level, mask in focus_masks.items():
                mask_area = np.sum(mask) / (h * w) * 100
                analysis[f'{level}_attention_area'] = mask_area
                
                if np.any(mask):
                    # Color analysis in focused regions
                    focus_colors = cv2.mean(image, mask=mask)
                    analysis[f'{level}_dominant_color_rgb'] = focus_colors[:3]
                    
                    # Texture analysis
                    focus_texture = cv2.Laplacian(gray_image, cv2.CV_64F, ksize=3)
                    analysis[f'{level}_texture_variance'] = np.var(focus_texture[mask > 0])
                else:
                    analysis[f'{level}_dominant_color_rgb'] = (128, 128, 128)
                    analysis[f'{level}_texture_variance'] = 0
            
            # üî• Advanced feature analysis
            # Edge density and distribution
            edges = cv2.Canny(gray_image, 50, 150)
            analysis['global_edge_density'] = np.mean(edges)
            
            # Focused edge density
            if np.any(focus_masks['high']):
                analysis['focused_edge_density'] = np.mean(edges[focus_masks['high'] > 0])
            else:
                analysis['focused_edge_density'] = 0
            
            # Contrast analysis
            analysis['global_contrast'] = np.std(gray_image)
            analysis['focused_contrast'] = np.std(gray_image[focus_masks['high'] > 0]) if np.any(focus_masks['high']) else 0
            
            # üî• Attention pattern analysis
            analysis['attention_concentration'] = self._analyze_attention_concentration(heatmap)
            analysis['attention_symmetry'] = self._analyze_attention_symmetry(heatmap)
            analysis['attention_dispersion'] = self._analyze_attention_dispersion(heatmap)
            
            return analysis
            
        except Exception as e:
            print(f"Warning: Enhanced content analysis failed: {e}")
            return self._analyze_image_content_basic(image, heatmap)  # Fallback to basic analysis

    def _analyze_image_content_basic(self, image: np.ndarray, heatmap: np.ndarray) -> Dict:
        """Basic fallback image content analysis"""
        analysis = {}
        
        # Basic color analysis
        mean_color = np.mean(image, axis=(0, 1))
        analysis['dominant_colors'] = mean_color.tolist()
        
        # Basic texture analysis
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if CV2_AVAILABLE else np.mean(image, axis=2)
        analysis['texture_strength'] = float(np.std(gray))
        analysis['brightness'] = float(np.mean(gray))
        
        # Basic attention analysis
        analysis['attention_concentration'] = float(np.max(heatmap))
        analysis['attention_coverage'] = float(np.mean(heatmap > 0.5))
        
        # Default quality scores
        analysis['stitching_quality'] = 0.5
        analysis['logo_sharpness'] = 0.5
        analysis['surface_roughness'] = 0.5
        analysis['shine_ratio'] = 0.3
        analysis['color_vibrancy'] = 0.6
        
        return analysis

    def _analyze_material_properties(self, image: np.ndarray, focus_masks: Dict) -> Dict:
        """üî• Analyze material properties like shine, texture, fabric quality"""
        analysis = {}
        
        # Convert to HSV for better material analysis
        hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
        
        # Analyze reflectivity/shine in high attention areas
        if np.any(focus_masks['high']):
            high_attention_pixels = image[focus_masks['high'] > 0]
            
            # Shine analysis (bright spots)
            brightness = np.mean(high_attention_pixels, axis=1)
            shine_pixels = np.sum(brightness > 200)
            total_pixels = len(brightness)
            analysis['shine_ratio'] = shine_pixels / max(total_pixels, 1)
            
            # Material uniformity
            color_std = np.std(high_attention_pixels, axis=0)
            analysis['material_uniformity'] = 1.0 / (1.0 + np.mean(color_std) / 50.0)
            
            # Surface smoothness (local standard deviation)
            gray_roi = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)[focus_masks['high'] > 0]
            analysis['surface_smoothness'] = 1.0 / (1.0 + np.std(gray_roi) / 30.0)
        else:
            analysis['shine_ratio'] = 0.0
            analysis['material_uniformity'] = 0.5
            analysis['surface_smoothness'] = 0.5
            
        return analysis

    def _analyze_manufacturing_details(self, image: np.ndarray, gray_image: np.ndarray, focus_masks: Dict) -> Dict:
        """üî• Analyze manufacturing quality indicators"""
        analysis = {}
        
        try:
            if np.any(focus_masks['high']):
                # Stitching quality analysis
                edges = cv2.Canny(gray_image, 30, 100)
                high_edges = edges[focus_masks['high'] > 0]
                
                # Count line-like structures (stitching)
                kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 15))
                vertical_lines = cv2.morphologyEx(edges, cv2.MORPH_OPEN, kernel)
                horizontal_lines = cv2.morphologyEx(edges, cv2.MORPH_OPEN, kernel.T)
                
                analysis['stitching_quality'] = (np.sum(vertical_lines) + np.sum(horizontal_lines)) / max(np.sum(edges), 1)
                
                # Print/pattern regularity - FIXED
                roi = gray_image[focus_masks['high'] > 0]
                if len(roi) > 10000:  # Need enough pixels for analysis
                    # Find the largest square size we can make
                    side_length = int(np.sqrt(len(roi)))
                    if side_length > 10:  # Minimum meaningful size
                        roi_square = roi[:side_length*side_length].reshape(side_length, side_length)
                        fft_roi = np.fft.fft2(roi_square)
                        power_spectrum = np.abs(fft_roi) ** 2
                        analysis['pattern_regularity'] = np.std(power_spectrum) / (np.mean(power_spectrum) + 1e-6)
                    else:
                        analysis['pattern_regularity'] = 0.5
                else:
                    analysis['pattern_regularity'] = 0.5
                    
                # Logo/text sharpness
                logo_edges = cv2.Canny(gray_image, 100, 200)
                logo_density = np.sum(logo_edges[focus_masks['high'] > 0]) / max(np.sum(focus_masks['high']), 1)
                analysis['logo_sharpness'] = min(logo_density / 0.1, 1.0)  # Normalize
            else:
                analysis['stitching_quality'] = 0.5
                analysis['pattern_regularity'] = 0.5
                analysis['logo_sharpness'] = 0.5
                
        except Exception as e:
            print(f"Warning: Manufacturing analysis failed: {e}")
            analysis['stitching_quality'] = 0.5
            analysis['pattern_regularity'] = 0.5
            analysis['logo_sharpness'] = 0.5
        
        return analysis

    def _analyze_color_patterns(self, image: np.ndarray, focus_masks: Dict) -> Dict:
        """üî• Analyze color accuracy and consistency"""
        analysis = {}
        
        if np.any(focus_masks['high']):
            high_roi = image[focus_masks['high'] > 0]
            
            # Color vibrancy
            hsv_roi = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)[focus_masks['high'] > 0]
            saturation = hsv_roi[:, 1] if len(hsv_roi.shape) > 1 else hsv_roi
            analysis['color_vibrancy'] = np.mean(saturation) / 255.0
            
            # Color bleeding (transition smoothness)
            if len(high_roi) > 50:
                # Analyze color gradients
                grad_x = np.abs(np.diff(high_roi, axis=0)) if high_roi.shape[0] > 1 else np.array([0])
                grad_y = np.abs(np.diff(high_roi, axis=1)) if len(high_roi.shape) > 1 and high_roi.shape[1] > 1 else np.array([0])
                analysis['color_bleeding'] = (np.mean(grad_x) + np.mean(grad_y)) / 2.0 / 50.0
            else:
                analysis['color_bleeding'] = 0.0
                
            # Dominant color analysis
            dominant_colors = self._find_dominant_colors(high_roi)
            analysis['num_dominant_colors'] = len(dominant_colors)
            analysis['color_complexity'] = len(dominant_colors) / 10.0  # Normalize to 0-1
        else:
            analysis['color_vibrancy'] = 0.0
            analysis['color_bleeding'] = 0.0
            analysis['num_dominant_colors'] = 0
            analysis['color_complexity'] = 0.0
            
        return analysis

    def _analyze_geometric_features(self, image: np.ndarray, gray_image: np.ndarray, focus_masks: Dict) -> Dict:
        """üî• Analyze geometric accuracy and proportions"""
        analysis = {}
        
        if np.any(focus_masks['high']):
            # Find contours in high attention area
            mask_uint8 = focus_masks['high'].astype(np.uint8) * 255
            contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            if contours:
                # Analyze shape regularity
                largest_contour = max(contours, key=cv2.contourArea)
                
                # Circularity (how close to perfect circle)
                area = cv2.contourArea(largest_contour)
                perimeter = cv2.arcLength(largest_contour, True)
                if perimeter > 0:
                    circularity = 4 * np.pi * area / (perimeter * perimeter)
                    analysis['shape_regularity'] = min(circularity, 1.0)
                else:
                    analysis['shape_regularity'] = 0.0
                    
                # Symmetry analysis
                moments = cv2.moments(largest_contour)
                if moments['m00'] > 0:
                    cx = int(moments['m10'] / moments['m00'])
                    cy = int(moments['m01'] / moments['m00'])
                    analysis['geometric_symmetry'] = self._calculate_contour_symmetry(largest_contour, cx, cy)
                else:
                    analysis['geometric_symmetry'] = 0.0
            else:
                analysis['shape_regularity'] = 0.0
                analysis['geometric_symmetry'] = 0.0
                
            # Logo/text alignment
            edges = cv2.Canny(gray_image, 50, 150)
            lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=50, minLineLength=30, maxLineGap=10)
            if lines is not None:
                angles = []
                for line in lines:
                    x1, y1, x2, y2 = line[0]
                    angle = np.arctan2(y2 - y1, x2 - x1)
                    angles.append(angle)
                analysis['alignment_score'] = 1.0 - (np.std(angles) / np.pi if angles else 0)
            else:
                analysis['alignment_score'] = 0.5
        else:
            analysis['shape_regularity'] = 0.0
            analysis['geometric_symmetry'] = 0.0
            analysis['alignment_score'] = 0.0
            
        return analysis

    def _analyze_surface_texture(self, gray_image: np.ndarray, focus_masks: Dict) -> Dict:
        """üî• Deep texture analysis for authenticity"""
        analysis = {}
        
        if np.any(focus_masks['high']):
            roi = gray_image[focus_masks['high'] > 0]
            
            # Local Binary Pattern for texture
            if len(roi) > 100:
                roi_2d = roi.reshape(int(np.sqrt(len(roi))), -1)[:int(np.sqrt(len(roi)))]
                
                # Texture directionality
                grad_x = cv2.Sobel(roi_2d, cv2.CV_64F, 1, 0, ksize=3)
                grad_y = cv2.Sobel(roi_2d, cv2.CV_64F, 0, 1, ksize=3)
                
                magnitude = np.sqrt(grad_x**2 + grad_y**2)
                direction = np.arctan2(grad_y, grad_x)
                
                analysis['texture_directionality'] = np.std(direction)
                analysis['texture_strength'] = np.mean(magnitude) / 50.0  # Normalize
                
                # Surface roughness
                laplacian = cv2.Laplacian(roi_2d, cv2.CV_64F)
                analysis['surface_roughness'] = np.var(laplacian) / 1000.0  # Normalize
            else:
                analysis['texture_directionality'] = 0.0
                analysis['texture_strength'] = 0.0
                analysis['surface_roughness'] = 0.0
        else:
            analysis['texture_directionality'] = 0.0
            analysis['texture_strength'] = 0.0
            analysis['surface_roughness'] = 0.0
            
        return analysis

    def _find_dominant_colors(self, image_roi: np.ndarray, k: int = 5) -> List:
        """Find dominant colors in image region"""
        try:
            data = image_roi.reshape((-1, 3))
            data = np.float32(data)
            
            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
            _, labels, centers = cv2.kmeans(data, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
            
            # Count occurrences of each color
            unique_labels, counts = np.unique(labels, return_counts=True)
            dominant_colors = []
            
            for i, count in enumerate(counts):
                if count > len(data) * 0.05:  # At least 5% of pixels
                    dominant_colors.append(centers[unique_labels[i]])
                    
            return dominant_colors
        except:
            return []

    def _calculate_contour_symmetry(self, contour: np.ndarray, cx: int, cy: int) -> float:
        """Calculate symmetry score of a contour around center point"""
        try:
            # Create mask for left and right halves
            rect = cv2.boundingRect(contour)
            x, y, w, h = rect
            
            # Simple horizontal symmetry check
            left_half = contour[contour[:, :, 0] < cx]
            right_half = contour[contour[:, :, 0] >= cx]
            
            if len(left_half) == 0 or len(right_half) == 0:
                return 0.5
                
            # Mirror left half and compare with right half
            left_mirrored = left_half.copy()
            left_mirrored[:, :, 0] = 2 * cx - left_mirrored[:, :, 0]
            
            # Calculate similarity (simplified)
            return min(len(left_half), len(right_half)) / max(len(left_half), len(right_half))
        except:
            return 0.5

    def _analyze_attention_concentration(self, heatmap: np.ndarray) -> float:
        """Measure how concentrated the attention is (0=dispersed, 1=highly concentrated)"""
        # Calculate entropy of the heatmap
        heatmap_flat = heatmap.flatten()
        heatmap_normalized = heatmap_flat / (np.sum(heatmap_flat) + 1e-8)
        entropy = -np.sum(heatmap_normalized * np.log(heatmap_normalized + 1e-8))
        max_entropy = np.log(len(heatmap_normalized))
        return 1.0 - (entropy / max_entropy) if max_entropy > 0 else 0.5

    def _analyze_attention_symmetry(self, heatmap: np.ndarray) -> float:
        """Measure attention symmetry (0=asymmetric, 1=perfectly symmetric)"""
        # Compare left and right halves
        h, w = heatmap.shape
        left_half = heatmap[:, :w//2]
        right_half = np.fliplr(heatmap[:, w//2:])
        
        # Resize to match if needed
        min_width = min(left_half.shape[1], right_half.shape[1])
        left_half = left_half[:, :min_width]
        right_half = right_half[:, :min_width]
        
        # Calculate correlation
        correlation = np.corrcoef(left_half.flatten(), right_half.flatten())[0, 1]
        return max(0, correlation) if not np.isnan(correlation) else 0.5

    def _analyze_attention_dispersion(self, heatmap: np.ndarray) -> float:
        """Measure how dispersed the attention is across the image"""
        # Calculate weighted centroid
        h, w = heatmap.shape
        y_coords, x_coords = np.mgrid[0:h, 0:w]
        
        total_attention = np.sum(heatmap)
        if total_attention == 0:
            return 0.5
        
        centroid_y = np.sum(y_coords * heatmap) / total_attention
        centroid_x = np.sum(x_coords * heatmap) / total_attention
        
        # Calculate average distance from centroid
        distances = np.sqrt((y_coords - centroid_y)**2 + (x_coords - centroid_x)**2)
        avg_distance = np.sum(distances * heatmap) / total_attention
        
        # Normalize by image diagonal
        max_distance = np.sqrt(h**2 + w**2)
        return avg_distance / max_distance

    def _generate_vietnamese_compact(self, prediction: str, confidence: float, 
                                   analysis: Dict, attention_results: Dict) -> str:
        """SIMPLE VIETNAMESE ANALYSIS"""
        
        conf_percent = round(confidence * 100, 1)
        is_real = prediction.lower() == 'real'
        
        if is_real:
            text = f"S·∫£n ph·∫©m ch√≠nh h√£ng - ƒê·ªô tin c·∫≠y {conf_percent}%\n\n"
            text += f"ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng cao v·ªõi ƒë·ªô ch√≠nh x√°c {conf_percent}%. "
            text += "C√°c ƒë·∫∑c ƒëi·ªÉm nh·∫≠n d·∫°ng ƒë·ªÅu kh·ªõp v·ªõi ti√™u chu·∫©n g·ªëc. "
            text += "Kh√¥ng ph√°t hi·ªán d·∫•u hi·ªáu b·∫•t th∆∞·ªùng."
        else:
            text = f"S·∫£n ph·∫©m giaÃâ - ƒê·ªô tin c·∫≠y {conf_percent}%\n\n"
            text += f"Ph√°t hi·ªán nhi·ªÅu b·∫•t th∆∞·ªùng v·ªõi ƒë·ªô ch√≠nh x√°c {conf_percent}%. "
            text += "Ch·∫•t l∆∞·ª£ng k√©m v√† kh√¥ng ƒë·∫°t ti√™u chu·∫©n. "
            text += "Khuy·∫øn ngh·ªã kh√¥ng s·ª≠ d·ª•ng s·∫£n ph·∫©m n√†y."
        
        return text

    def _generate_enhanced_explanation(self, prediction: str, confidence: float, 
                                     analysis: Dict, attention_results: Dict, 
                                     product_data: Dict = None) -> str:
        """üî• COMPLETELY DYNAMIC: Generate explanation based on ACTUAL image analysis"""
        
        # üî• FORCE NEW DYNAMIC ANALYSIS
        print("üî•üî•üî• USING COMPLETELY NEW DYNAMIC GENERATOR!")
        
        # üî• NUCLEAR OPTION - RETURN PURE RAW DATA
        explanation = f"ÔøΩ **NUCLEAR TEST: {prediction.upper()}** (Confidence: {confidence:.1%})\n\n"
        
        explanation += "üö® **THIS IS A COMPLETELY NEW EXPLANATION SYSTEM!**\n\n"
        
        # RAW DATA DUMP
        explanation += "üìä **RAW ANALYSIS DATA:**\n"
        for key, value in analysis.items():
            if isinstance(value, (int, float)):
                explanation += f"‚Ä¢ {key}: {value:.4f}\n"
        
        explanation += f"\nü§ñ **PREDICTION:** {prediction}\n"
        explanation += f"üéØ **CONFIDENCE:** {confidence:.1%}\n"
        explanation += f"‚ö° **TIMESTAMP:** {__import__('datetime').datetime.now()}\n"
        
        explanation += "\nüî• **IF YOU SEE THIS TEXT, THE NEW SYSTEM IS WORKING!**"
        
        # üî• REAL DATA DRIVEN ANALYSIS
        explanation += "ÔøΩ **D·ªØ Li·ªáu Th·ª±c T·∫ø Ph√¢n T√≠ch:**\n"
        explanation += self._generate_real_data_summary(analysis)
        
        # üî• DYNAMIC VISUAL FINDINGS
        explanation += "\nüëÅÔ∏è **Ph√°t Hi·ªán Tr·ª±c Ti·∫øp:**\n"
        explanation += self._generate_dynamic_findings(analysis, prediction.lower() == 'fake')
        
        # üî• MEASUREMENT-BASED CONCLUSION
        explanation += "\nüìè **K·∫øt Lu·∫≠n D·ª±a Tr√™n ƒêo L∆∞·ªùng:**\n"
        explanation += self._generate_measurement_conclusion(analysis, confidence, prediction.lower() == 'fake')
        
        # üî• ATTENTION ANALYSIS
        explanation += "\nüéØ **Ph√¢n T√≠ch Attention Pattern:**\n"
        explanation += self._analyze_attention_pattern(analysis, attention_results)
        
        return explanation

    def _generate_real_data_summary(self, analysis: Dict) -> str:
        """üî• NEW: Generate summary from actual measurement data"""
        summary = []
        
        # Show raw measurements that were actually calculated
        measurements = []
        for key, value in analysis.items():
            if isinstance(value, (int, float)) and key not in ['high_attention_area', 'medium_attention_area', 'low_attention_area']:
                if 'color' in key.lower() and isinstance(value, list):
                    continue  # Skip color arrays
                measurements.append(f"  - {key}: {value:.3f}" if isinstance(value, float) else f"  - {key}: {value}")
        
        if measurements:
            summary.append("üìä **C√°c ch·ªâ s·ªë ƒëo ƒë∆∞·ª£c:**")
            summary.extend(measurements[:8])  # Limit to top 8 measurements
        
        # Show actual attention distribution
        high_attn = analysis.get('high_attention_area', 0)
        med_attn = analysis.get('medium_attention_area', 0)
        low_attn = analysis.get('low_attention_area', 0)
        
        if high_attn > 0 or med_attn > 0 or low_attn > 0:
            summary.append(f"üéØ **Ph√¢n b·ªë attention:** High={high_attn:.1f}%, Med={med_attn:.1f}%, Low={low_attn:.1f}%")
        
        return "\n".join(summary) if summary else "‚Ä¢ Kh√¥ng thu th·∫≠p ƒë∆∞·ª£c d·ªØ li·ªáu ƒëo l∆∞·ªùng"

    def _generate_dynamic_findings(self, analysis: Dict, is_fake: bool) -> str:
        """üî• NEW: Generate completely dynamic findings based on actual data"""
        findings = []
        
        # Analyze shine ratio with dynamic interpretation
        shine_ratio = analysis.get('shine_ratio', 0)
        if shine_ratio > 0:
            shine_percent = shine_ratio * 100
            if shine_percent > 50:
                findings.append(f"‚Ä¢ Ph√°t hi·ªán **{shine_percent:.0f}% b·ªÅ m·∫∑t ph·∫£n quang** - ch·∫•t li·ªáu synthetic/da th·∫≠t")
            elif shine_percent > 20:
                findings.append(f"‚Ä¢ **{shine_percent:.0f}% v√πng b√≥ng nh·∫π** - ch·∫•t li·ªáu semi-matte")
            elif shine_percent > 5:
                findings.append(f"‚Ä¢ **{shine_percent:.0f}% ƒëi·ªÉm ph·∫£n chi·∫øu** - texture nh·∫π")
            else:
                findings.append(f"‚Ä¢ **{shine_percent:.0f}% ph·∫£n quang** - ho√†n to√†n matte")
        
        # Dynamic texture analysis
        texture_strength = analysis.get('texture_strength', 0)
        surface_roughness = analysis.get('surface_roughness', 0)
        
        if texture_strength > 0:
            if texture_strength > 80:
                findings.append(f"‚Ä¢ **Texture c∆∞·ªùng ƒë·ªô {texture_strength:.0f}** - v√¢n r·∫•t r√µ (da th·∫≠t/canvas d√†y)")
            elif texture_strength > 50:
                findings.append(f"‚Ä¢ **Texture c∆∞·ªùng ƒë·ªô {texture_strength:.0f}** - c√≥ v√¢n trung b√¨nh")
            elif texture_strength > 20:
                findings.append(f"‚Ä¢ **Texture c∆∞·ªùng ƒë·ªô {texture_strength:.0f}** - v√¢n nh·∫π")
            else:
                findings.append(f"‚Ä¢ **Texture c∆∞·ªùng ƒë·ªô {texture_strength:.0f}** - b·ªÅ m·∫∑t g·∫ßn nh∆∞ m·ªãn")
        
        # Dynamic edge analysis
        edge_density = analysis.get('focused_edge_density', 0)
        global_edge = analysis.get('global_edge_density', 0)
        
        if edge_density > 0:
            edge_ratio = edge_density / max(global_edge, 1)
            if edge_ratio > 2:
                findings.append(f"‚Ä¢ **Edge density focus {edge_ratio:.1f}x** - c√≥ logo/pattern t·∫≠p trung")
            elif edge_ratio > 1.3:
                findings.append(f"‚Ä¢ **Edge density {edge_ratio:.1f}x** - chi ti·∫øt v·ª´a ph·∫£i")
            else:
                findings.append(f"‚Ä¢ **Edge density uniform** - kh√¥ng c√≥ ƒëi·ªÉm nh·∫•n")
        
        # Dynamic color analysis
        dominant_colors = analysis.get('dominant_colors', [])
        if len(dominant_colors) >= 3:
            r, g, b = dominant_colors[:3]
            total_brightness = r + g + b
            if total_brightness > 600:
                findings.append(f"‚Ä¢ **M√†u s√°ng** (R{r:.0f}G{g:.0f}B{b:.0f}) - tone cao, c√≥ th·ªÉ over-processed")
            elif total_brightness > 400:
                findings.append(f"‚Ä¢ **M√†u trung b√¨nh** (R{r:.0f}G{g:.0f}B{b:.0f}) - tone t·ª± nhi√™n")
            else:
                findings.append(f"‚Ä¢ **M√†u t·ªëi** (R{r:.0f}G{g:.0f}B{b:.0f}) - tone th·∫•p")
        
        # Material uniformity with dynamic interpretation
        uniformity = analysis.get('material_uniformity', 0)
        if uniformity > 0:
            if uniformity > 0.9:
                quality_desc = "c·ª±c k·ª≥ ƒë·ªìng ƒë·ªÅu (nghi ng·ªù machine-made)" if is_fake else "ch·∫•t l∆∞·ª£ng industrial cao"
                findings.append(f"‚Ä¢ **Material uniformity {uniformity:.3f}** - {quality_desc}")
            elif uniformity > 0.7:
                findings.append(f"‚Ä¢ **Material uniformity {uniformity:.3f}** - ch·∫•t l∆∞·ª£ng t·ªët")
            elif uniformity > 0.5:
                findings.append(f"‚Ä¢ **Material uniformity {uniformity:.3f}** - ch·∫•t l∆∞·ª£ng trung b√¨nh")
            else:
                findings.append(f"‚Ä¢ **Material uniformity {uniformity:.3f}** - kh√¥ng ·ªïn ƒë·ªãnh")
        
        return "\n".join(findings) if findings else "‚Ä¢ Kh√¥ng ph√°t hi·ªán ƒë·∫∑c ƒëi·ªÉm ƒë√°ng ch√∫ √Ω"

    def _generate_measurement_conclusion(self, analysis: Dict, confidence: float, is_fake: bool) -> str:
        """üî• NEW: Generate conclusion based purely on measurements"""
        conclusions = []
        
        # Calculate composite scores from actual measurements
        material_score = 0
        visual_score = 0
        tech_score = 0
        
        # Material composite
        material_factors = ['material_uniformity', 'surface_smoothness', 'shine_ratio']
        material_values = [analysis.get(f, 0) for f in material_factors if analysis.get(f, 0) > 0]
        if material_values:
            material_score = sum(material_values) / len(material_values)
        
        # Visual composite  
        visual_factors = ['logo_sharpness', 'focused_edge_density', 'global_edge_density']
        visual_values = [analysis.get(f, 0) for f in visual_factors if analysis.get(f, 0) > 0]
        if visual_values:
            visual_score = sum(visual_values) / len(visual_values) / 100  # Normalize edge density
        
        # Technical composite
        tech_factors = ['attention_concentration', 'attention_symmetry']
        tech_values = [analysis.get(f, 0) for f in tech_factors if analysis.get(f, 0) > 0]
        if tech_values:
            tech_score = sum(tech_values) / len(tech_values)
        
        # Generate dynamic conclusions
        if material_score > 0:
            if material_score > 0.8:
                conclusions.append(f"‚Ä¢ **Material Score: {material_score:.2f}/1.0** - {'Qu√° ho√†n h·∫£o (nghi ng·ªù)' if is_fake and material_score > 0.95 else 'Ch·∫•t l∆∞·ª£ng cao'}")
            elif material_score > 0.6:
                conclusions.append(f"‚Ä¢ **Material Score: {material_score:.2f}/1.0** - Ch·∫•t l∆∞·ª£ng trung b√¨nh t·ªët")
            else:
                conclusions.append(f"‚Ä¢ **Material Score: {material_score:.2f}/1.0** - Ch·∫•t l∆∞·ª£ng th·∫•p")
        
        if visual_score > 0:
            if visual_score > 0.8:
                conclusions.append(f"‚Ä¢ **Visual Score: {visual_score:.2f}/1.0** - Chi ti·∫øt r·∫•t s·∫Øc n√©t")
            elif visual_score > 0.5:
                conclusions.append(f"‚Ä¢ **Visual Score: {visual_score:.2f}/1.0** - Chi ti·∫øt ·ªïn")
            else:
                conclusions.append(f"‚Ä¢ **Visual Score: {visual_score:.2f}/1.0** - Chi ti·∫øt m·ªù")
        
        # Confidence correlation analysis
        score_avg = (material_score + visual_score + tech_score) / 3 if any([material_score, visual_score, tech_score]) else 0
        confidence_gap = abs(confidence - score_avg)
        
        if confidence_gap > 0.3:
            conclusions.append(f"‚Ä¢ **Confidence Gap: {confidence_gap:.2f}** - Model c√≥ th·ªÉ ƒëang d·ª±a v√†o features kh√°c")
        elif confidence_gap < 0.1:
            conclusions.append(f"‚Ä¢ **Confidence Match: {confidence_gap:.2f}** - Prediction nh·∫•t qu√°n v·ªõi measurements")
        
        # Final dynamic assessment
        if is_fake:
            if confidence > 0.8 and material_score > 0.8:
                conclusions.append("üö® **Fake ch·∫•t l∆∞·ª£ng cao** - metrics t·ªët nh∆∞ng model detect patterns ·∫©n")
            elif confidence > 0.7:
                conclusions.append("‚ö†Ô∏è **C√≥ d·∫•u hi·ªáu fake** - m·ªôt s·ªë indicators kh√¥ng ƒë√∫ng")
            else:
                conclusions.append("ü§î **Nghi ng·ªù fake** - c·∫ßn th√™m evidence")
        else:
            if confidence > 0.9 and material_score > 0.7:
                conclusions.append("‚úÖ **Ch·∫Øc ch·∫Øn authentic** - t·∫•t c·∫£ metrics ƒë·ªÅu t·ªët")
            elif confidence > 0.8:
                conclusions.append("‚úÖ **C√≥ th·ªÉ authentic** - ƒëa s·ªë indicators t√≠ch c·ª±c")
            else:
                conclusions.append("ü§î **C·∫ßn xem x√©t th√™m** - m·ªôt s·ªë indicators m√¢u thu·∫´n")
        
        return "\n".join(conclusions) if conclusions else "‚Ä¢ Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ k·∫øt lu·∫≠n"

    def _analyze_attention_pattern(self, analysis: Dict, attention_results: Dict) -> str:
        """üî• NEW: Analyze actual attention patterns"""
        pattern_analysis = []
        
        concentration = analysis.get('attention_concentration', 0)
        symmetry = analysis.get('attention_symmetry', 0)
        dispersion = analysis.get('attention_dispersion', 0)
        
        # Attention concentration analysis
        if concentration > 0:
            if concentration > 0.8:
                pattern_analysis.append(f"‚Ä¢ **Focus c·ª±c cao ({concentration:.2f})** - AI t√¨m th·∫•y 1 ƒëi·ªÉm r·∫•t quan tr·ªçng")
            elif concentration > 0.6:
                pattern_analysis.append(f"‚Ä¢ **Focus t·∫≠p trung ({concentration:.2f})** - AI ch√∫ √Ω v√†i v√πng ch√≠nh")
            elif concentration > 0.4:
                pattern_analysis.append(f"‚Ä¢ **Focus ph√¢n t√°n ({concentration:.2f})** - AI scan nhi·ªÅu v√πng")
            else:
                pattern_analysis.append(f"‚Ä¢ **Focus ƒë·ªìng ƒë·ªÅu ({concentration:.2f})** - AI xem to√†n b·ªô ·∫£nh")
        
        # Symmetry analysis
        if symmetry > 0:
            if symmetry > 0.8:
                pattern_analysis.append(f"‚Ä¢ **Attention ƒë·ªëi x·ª©ng ({symmetry:.2f})** - pattern symmetric")
            elif symmetry > 0.5:
                pattern_analysis.append(f"‚Ä¢ **Attention c√¢n b·∫±ng ({symmetry:.2f})** - pattern balanced")
            else:
                pattern_analysis.append(f"‚Ä¢ **Attention l·ªách ({symmetry:.2f})** - pattern asymmetric")
        
        # Method analysis
        methods = list(attention_results.keys())
        if methods:
            pattern_analysis.append(f"‚Ä¢ **Methods used:** {', '.join(methods)}")
        
        return "\n".join(pattern_analysis) if pattern_analysis else "‚Ä¢ Kh√¥ng ph√¢n t√≠ch ƒë∆∞·ª£c attention pattern"

    def _get_confidence_descriptor(self, confidence: float) -> str:
        """Get descriptive confidence level"""
        if confidence >= 0.95:
            return "C·ª±c K·ª≥ Ch·∫Øc Ch·∫Øn"
        elif confidence >= 0.85:
            return "R·∫•t Ch·∫Øc Ch·∫Øn"
        elif confidence >= 0.75:
            return "Kh√° Ch·∫Øc Ch·∫Øn"
        elif confidence >= 0.65:
            return "T∆∞∆°ng ƒê·ªëi Ch·∫Øc Ch·∫Øn"
        elif confidence >= 0.55:
            return "Kh√¥ng Ch·∫Øc Ch·∫Øn"
        else:
            return "R·∫•t Kh√¥ng Ch·∫Øc Ch·∫Øn"

    def _describe_visual_observations(self, analysis: Dict, attention_results: Dict) -> str:
        """üî• REAL: Describe what AI actually sees in THIS specific image"""
        observations = []
        
        # Analyze actual attention pattern
        attention_concentration = analysis.get('attention_concentration', 0)
        attention_symmetry = analysis.get('attention_symmetry', 0)
        
        if attention_concentration > 0.8:
            observations.append("‚Ä¢ AI ph√°t hi·ªán **m·ªôt v√πng c·ª• th·ªÉ r·∫•t ƒë√°ng ch√∫ √Ω** trong ·∫£nh")
        elif attention_concentration > 0.5:
            observations.append("‚Ä¢ AI nh·∫≠n ra **v√†i khu v·ª±c quan tr·ªçng** c·∫ßn ph√¢n t√≠ch k·ªπ")
        else:
            observations.append("‚Ä¢ AI c·∫ßn **qu√©t to√†n b·ªô ·∫£nh** ƒë·ªÉ ƒë∆∞a ra k·∫øt lu·∫≠n")
            
        # Real material analysis based on actual image content
        shine_ratio = analysis.get('shine_ratio', 0)
        surface_roughness = analysis.get('surface_roughness', 0.5)
        
        if shine_ratio > 0.4:
            observations.append(f"‚Ä¢ B·ªÅ m·∫∑t c√≥ **{shine_ratio*100:.0f}% v√πng b√≥ng** - ch·∫•t li·ªáu da/plastic")
        elif shine_ratio > 0.15:
            observations.append(f"‚Ä¢ C√≥ **{shine_ratio*100:.0f}% v√πng nh·∫π b√≥ng** - ch·∫•t li·ªáu h·ªón h·ª£p")
        else:
            observations.append("‚Ä¢ B·ªÅ m·∫∑t **ho√†n to√†n matte** - v·∫£i cotton/canvas")
            
        # Real texture analysis
        texture_strength = analysis.get('texture_strength', 0)
        if texture_strength > 60:
            observations.append(f"‚Ä¢ Texture c√≥ **ƒë·ªô t∆∞∆°ng ph·∫£n {texture_strength:.0f}** - th·∫•y r√µ v√¢n da/s·ª£i v·∫£i")
        elif texture_strength > 30:
            observations.append(f"‚Ä¢ Texture **trung b√¨nh ({texture_strength:.0f})** - c√≥ pattern nh∆∞ng m·ªãn")
        else:
            observations.append(f"‚Ä¢ B·ªÅ m·∫∑t **r·∫•t m·ªãn ({texture_strength:.0f})** - g·∫ßn nh∆∞ kh√¥ng c√≥ texture")
        
        # Real color analysis based on actual dominant colors
        dominant_colors = analysis.get('dominant_colors', [])
        if len(dominant_colors) >= 3:
            r, g, b = dominant_colors[:3]
            if max(r, g, b) > 200:
                observations.append(f"‚Ä¢ M√†u s·∫Øc **s√°ng** (RGB: {r:.0f}, {g:.0f}, {b:.0f}) - tone m√†u cao")
            elif max(r, g, b) < 100:
                observations.append(f"‚Ä¢ M√†u s·∫Øc **t·ªëi** (RGB: {r:.0f}, {g:.0f}, {b:.0f}) - tone m√†u th·∫•p")
            else:
                observations.append(f"‚Ä¢ M√†u s·∫Øc **trung b√¨nh** (RGB: {r:.0f}, {g:.0f}, {b:.0f}) - tone c√¢n b·∫±ng")
        
        # Real edge analysis
        focused_edge_density = analysis.get('focused_edge_density', 0)
        global_edge_density = analysis.get('global_edge_density', 0)
        
        if focused_edge_density > global_edge_density * 1.5:
            observations.append(f"‚Ä¢ V√πng ch√∫ √Ω c√≥ **nhi·ªÅu chi ti·∫øt** ({focused_edge_density:.1f} vs {global_edge_density:.1f}) - logo/pattern r√µ n√©t")
        elif focused_edge_density > 0:
            observations.append(f"‚Ä¢ Chi ti·∫øt **v·ª´a ph·∫£i** trong v√πng quan tr·ªçng ({focused_edge_density:.1f})")
        else:
            observations.append("‚Ä¢ **√çt chi ti·∫øt** trong v√πng AI quan t√¢m - b·ªÅ m·∫∑t ƒë·ªìng nh·∫•t")
            
        # Real manufacturing analysis
        stitching_quality = analysis.get('stitching_quality', 0)
        if stitching_quality > 0.3:
            observations.append(f"‚Ä¢ Ph√°t hi·ªán **ƒë∆∞·ªùng kh√¢u** v·ªõi ch·∫•t l∆∞·ª£ng {stitching_quality:.2f}")
        
        logo_sharpness = analysis.get('logo_sharpness', 0)
        if logo_sharpness > 0.5:
            observations.append(f"‚Ä¢ Logo/text c√≥ **ƒë·ªô s·∫Øc n√©t {logo_sharpness:.2f}** - in ·∫•n ch·∫•t l∆∞·ª£ng cao")
        elif logo_sharpness > 0.1:
            observations.append(f"‚Ä¢ Logo/text **ƒë·ªô s·∫Øc n√©t {logo_sharpness:.2f}** - in ·∫•n trung b√¨nh")
        
        return "\n".join(observations)

    def _analyze_material_quality(self, analysis: Dict) -> str:
        """üî• REAL: Analyze what we actually detect in THIS image"""
        material_analysis = []
        
        # Real uniformity measurement
        material_uniformity = analysis.get('material_uniformity', 0.5)
        surface_smoothness = analysis.get('surface_smoothness', 0.5)
        
        if material_uniformity > 0.85:
            material_analysis.append(f"‚Ä¢ **ƒê·ªô ƒë·ªìng ƒë·ªÅu {material_uniformity:.2f}** - ch·∫•t li·ªáu r·∫•t nh·∫•t qu√°n (ch√≠nh h√£ng)")
        elif material_uniformity > 0.65:
            material_analysis.append(f"‚Ä¢ **ƒê·ªô ƒë·ªìng ƒë·ªÅu {material_uniformity:.2f}** - ch·∫•t li·ªáu ·ªïn ƒë·ªãnh (ch·∫•p nh·∫≠n ƒë∆∞·ª£c)")
        else:
            material_analysis.append(f"‚Ä¢ **ƒê·ªô ƒë·ªìng ƒë·ªÅu {material_uniformity:.2f}** - ch·∫•t li·ªáu kh√¥ng ·ªïn ƒë·ªãnh (nghi ng·ªù)")
        
        # Real surface analysis
        if surface_smoothness > 0.8:
            material_analysis.append(f"‚Ä¢ **B·ªÅ m·∫∑t m·ªãn {surface_smoothness:.2f}** - gia c√¥ng cao c·∫•p")
        elif surface_smoothness > 0.5:
            material_analysis.append(f"‚Ä¢ **B·ªÅ m·∫∑t {surface_smoothness:.2f}** - gia c√¥ng ti√™u chu·∫©n")
        else:
            material_analysis.append(f"‚Ä¢ **B·ªÅ m·∫∑t th√¥ {surface_smoothness:.2f}** - gia c√¥ng k√©m ho·∫∑c v·∫≠t li·ªáu r·∫ª")
        
        # Real stitching analysis (if detected)
        stitching_quality = analysis.get('stitching_quality', 0)
        if stitching_quality > 0.1:  # Only mention if actually detected
            if stitching_quality > 0.6:
                material_analysis.append(f"‚Ä¢ **ƒê∆∞·ªùng may xu·∫•t s·∫Øc** (ch·ªâ s·ªë {stitching_quality:.2f}) - th·ª£ may chuy√™n nghi·ªáp")
            elif stitching_quality > 0.3:
                material_analysis.append(f"‚Ä¢ **ƒê∆∞·ªùng may t·ªët** (ch·ªâ s·ªë {stitching_quality:.2f}) - ti√™u chu·∫©n c√¥ng nghi·ªáp")
            else:
                material_analysis.append(f"‚Ä¢ **ƒê∆∞·ªùng may y·∫øu** (ch·ªâ s·ªë {stitching_quality:.2f}) - c√≥ th·ªÉ s·∫£n xu·∫•t k√©m")
        
        # Real color bleeding analysis
        color_bleeding = analysis.get('color_bleeding', 0)
        if color_bleeding > 0:  # Only if detected
            if color_bleeding < 20:
                material_analysis.append(f"‚Ä¢ **√çt lem m√†u** ({color_bleeding:.1f}) - nhu·ªôm ch·∫•t l∆∞·ª£ng")
            elif color_bleeding < 40:
                material_analysis.append(f"‚Ä¢ **Lem m√†u v·ª´a** ({color_bleeding:.1f}) - nhu·ªôm b√¨nh th∆∞·ªùng")
            else:
                material_analysis.append(f"‚Ä¢ **Lem m√†u nhi·ªÅu** ({color_bleeding:.1f}) - nhu·ªôm k√©m ch·∫•t l∆∞·ª£ng")
        
        # Pattern regularity (if detected)
        pattern_regularity = analysis.get('pattern_regularity', 0)
        if pattern_regularity > 0.1:  # Only if pattern exists
            if pattern_regularity > 0.7:
                material_analysis.append(f"‚Ä¢ **Pattern r·∫•t ƒë·ªÅu** ({pattern_regularity:.2f}) - in c√¥ng nghi·ªáp ch√≠nh x√°c")
            elif pattern_regularity > 0.4:
                material_analysis.append(f"‚Ä¢ **Pattern t∆∞∆°ng ƒë·ªëi ƒë·ªÅu** ({pattern_regularity:.2f}) - in ti√™u chu·∫©n")
            else:
                material_analysis.append(f"‚Ä¢ **Pattern kh√¥ng ƒë·ªÅu** ({pattern_regularity:.2f}) - in th·ªß c√¥ng ho·∫∑c l·ªói")
        
        # Real contrast analysis
        global_contrast = analysis.get('global_contrast', 0)
        focused_contrast = analysis.get('focused_contrast', 0)
        
        if global_contrast > 0:
            if global_contrast > 50:
                material_analysis.append(f"‚Ä¢ **ƒê·ªô t∆∞∆°ng ph·∫£n cao** ({global_contrast:.0f}) - chi ti·∫øt r√µ n√©t")
            elif global_contrast > 25:
                material_analysis.append(f"‚Ä¢ **ƒê·ªô t∆∞∆°ng ph·∫£n v·ª´a** ({global_contrast:.0f}) - chi ti·∫øt b√¨nh th∆∞·ªùng")
            else:
                material_analysis.append(f"‚Ä¢ **ƒê·ªô t∆∞∆°ng ph·∫£n th·∫•p** ({global_contrast:.0f}) - ·∫£nh m·ªù ho·∫∑c √°nh s√°ng y·∫øu")
        
        return "\n".join(material_analysis) if material_analysis else "‚Ä¢ Kh√¥ng ph√°t hi·ªán ƒë·∫∑c ƒëi·ªÉm ch·∫•t li·ªáu r√µ r√†ng trong ·∫£nh n√†y"

    def _identify_authenticity_markers(self, analysis: Dict, is_fake: bool) -> str:
        """üî• REAL: Identify specific markers based on actual measurements"""
        markers = []
        
        # Real geometric accuracy based on measurements
        shape_regularity = analysis.get('shape_regularity', 0.5)
        alignment_score = analysis.get('alignment_score', 0.5)
        
        if shape_regularity > 0 and alignment_score > 0:  # Only if we actually measured
            combined_score = (shape_regularity + alignment_score) / 2
            if combined_score > 0.85:
                markers.append(f"‚Ä¢ **Geometry score {combined_score:.2f}** - {'qu√° ho√†n h·∫£o (nghi ng·ªù copy)' if is_fake else 'ch√≠nh x√°c cao (authentic)'}")
            elif combined_score > 0.6:
                markers.append(f"‚Ä¢ **Geometry score {combined_score:.2f}** - {'t·ªët nh∆∞ng c√≥ l·ªói nh·ªè' if is_fake else 'ti√™u chu·∫©n c√¥ng nghi·ªáp'}")
            else:
                markers.append(f"‚Ä¢ **Geometry score {combined_score:.2f}** - {'s·∫£n xu·∫•t k√©m' if is_fake else 'handmade/vintage'}")
        
        # Real pattern analysis (only if pattern detected)
        pattern_regularity = analysis.get('pattern_regularity', 0)
        if pattern_regularity > 0.1:
            if pattern_regularity > 1.2:
                markers.append(f"‚Ä¢ **Pattern regularity {pattern_regularity:.2f}** - {'copy k·ªπ thu·∫≠t s·ªë' if is_fake else 'm√°y in hi·ªán ƒë·∫°i'}")
            elif pattern_regularity > 0.7:
                markers.append(f"‚Ä¢ **Pattern regularity {pattern_regularity:.2f}** - {'ch·∫•t l∆∞·ª£ng t·ªët' if not is_fake else 'fake ch·∫•t l∆∞·ª£ng cao'}")
            else:
                markers.append(f"‚Ä¢ **Pattern irregularity {pattern_regularity:.2f}** - {'in l·ªói' if is_fake else 'handprinted'}")
        
        # Real logo analysis (only if logo detected)
        logo_sharpness = analysis.get('logo_sharpness', 0)
        focused_edge_density = analysis.get('focused_edge_density', 0)
        
        if logo_sharpness > 0.1:  # Logo was actually detected
            if logo_sharpness > 0.8 and focused_edge_density > 50:
                markers.append(f"‚Ä¢ **Logo clarity {logo_sharpness:.2f}** & **edge density {focused_edge_density:.0f}** - {'scan ch·∫•t l∆∞·ª£ng cao' if is_fake else 'emboss/laser ch√≠nh h√£ng'}")
            elif logo_sharpness > 0.5:
                markers.append(f"‚Ä¢ **Logo clarity {logo_sharpness:.2f}** - {'in t·ªët' if not is_fake else 'fake quality decent'}")
            else:
                markers.append(f"‚Ä¢ **Logo blur {logo_sharpness:.2f}** - {'in k√©m' if is_fake else 'worn/vintage'}")
        
        # Real material consistency markers
        material_uniformity = analysis.get('material_uniformity', 0)
        surface_smoothness = analysis.get('surface_smoothness', 0)
        
        if material_uniformity > 0:
            material_score = (material_uniformity + surface_smoothness) / 2
            if material_score > 0.9:
                markers.append(f"‚Ä¢ **Material score {material_score:.2f}** - {'c·ª±c k·ª≥ ƒë·ªÅu (nghi ng·ªù synthetic)' if is_fake else 'ch·∫•t l∆∞·ª£ng cao c·∫•p'}")
            elif material_score > 0.7:
                markers.append(f"‚Ä¢ **Material score {material_score:.2f}** - {'ch·∫•t l∆∞·ª£ng t·ªët' if not is_fake else 'fake grade A'}")
            elif material_score > 0.4:
                markers.append(f"‚Ä¢ **Material score {material_score:.2f}** - {'ch·∫•t l∆∞·ª£ng trung b√¨nh' if not is_fake else 'fake grade B'}")
            else:
                markers.append(f"‚Ä¢ **Material score {material_score:.2f}** - {'ch·∫•t l∆∞·ª£ng k√©m' if is_fake else 'damaged/old'}")
        
        # Real color analysis
        color_vibrancy = analysis.get('color_vibrancy', 0)
        color_bleeding = analysis.get('color_bleeding', 0)
        
        if color_vibrancy > 0 and color_bleeding >= 0:
            if color_vibrancy > 0.8 and color_bleeding < 10:
                markers.append(f"‚Ä¢ **Color perfect** (vibrancy {color_vibrancy:.2f}, bleeding {color_bleeding:.0f}) - {'nghi ng·ªù digital print' if is_fake else 'nhu·ªôm chuy√™n nghi·ªáp'}")
            elif color_bleeding > 40:
                markers.append(f"‚Ä¢ **Color bleeding high** ({color_bleeding:.0f}) - {'dye cheap' if is_fake else 'natural aging'}")
        
        # Attention pattern analysis
        attention_concentration = analysis.get('attention_concentration', 0)
        if attention_concentration > 0.9:
            markers.append(f"‚Ä¢ **AI focus extreme** ({attention_concentration:.2f}) - c√≥ m·ªôt chi ti·∫øt r·∫•t ƒë√°ng ng·ªù")
        elif attention_concentration < 0.2:
            markers.append(f"‚Ä¢ **AI scan to√†n di·ªán** ({attention_concentration:.2f}) - kh√¥ng c√≥ ƒëi·ªÉm b·∫•t th∆∞·ªùng")
        
        return "\n".join(markers) if markers else "‚Ä¢ Kh√¥ng ph√°t hi·ªán d·∫•u hi·ªáu ƒë·∫∑c bi·ªát n√†o trong ·∫£nh n√†y"
        
        # Material consistency
        num_colors = analysis.get('num_dominant_colors', 0)
        if num_colors > 8:
            markers.append("‚Ä¢ **Qu√° nhi·ªÅu m√†u s·∫Øc** - c√≥ th·ªÉ l√† photo composite")
        elif num_colors < 2:
            markers.append("‚Ä¢ **Qu√° √≠t m√†u s·∫Øc** - c√≥ th·ªÉ thi·∫øu detail")
        
        # Surface texture authenticity
        texture_directionality = analysis.get('texture_directionality', 0)
        if texture_directionality > 2.5:
            markers.append("‚Ä¢ **Texture qu√° random** - c√≥ th·ªÉ l√† fake texture overlay")
        elif texture_directionality < 0.5:
            markers.append("‚Ä¢ **Texture ƒë·ªìng nh·∫•t** - s·∫£n xu·∫•t industrial")
        
        return "\n".join(markers) if markers else "‚Ä¢ Kh√¥ng ph√°t hi·ªán d·∫•u hi·ªáu ƒë·∫∑c bi·ªát r√µ r√†ng"

    def _explain_ai_reasoning(self, analysis: Dict, prediction: str, confidence: float, attention_results: Dict) -> str:
        """Explain the AI's reasoning process"""
        reasoning = []
        
        # Attention method used
        methods_used = list(attention_results.keys())
        if 'gradient' in methods_used:
            reasoning.append("‚Ä¢ S·ª≠ d·ª•ng **Gradient Analysis** - ph∆∞∆°ng ph√°p ch√≠nh x√°c nh·∫•t, theo d√µi gradient flows")
        if 'fused' in methods_used:
            reasoning.append("‚Ä¢ √Åp d·ª•ng **Multi-method Fusion** - k·∫øt h·ª£p nhi·ªÅu g√≥c ƒë·ªô ph√¢n t√≠ch")
        
        # Decision factors
        high_attention = analysis.get('high_attention_area', 0)
        concentration = analysis.get('attention_concentration', 0.5)
        
        if concentration > 0.7:
            reasoning.append(f"‚Ä¢ AI **t·∫≠p trung cao** ({concentration:.1%}) v√†o v√πng quan tr·ªçng - quy·∫øt ƒë·ªãnh d·ª±a tr√™n chi ti·∫øt c·ª• th·ªÉ")
        elif concentration < 0.3:
            reasoning.append(f"‚Ä¢ AI **ph√¢n t√°n attention** ({concentration:.1%}) - ƒë√°nh gi√° t·ªïng th·ªÉ to√†n ·∫£nh")
        
        # Confidence reasoning
        if confidence > 0.8:
            reasoning.append("‚Ä¢ **C√°c ch·ªâ s·ªë ƒë·ªìng thu·∫≠n** - nhi·ªÅu feature c√πng h∆∞·ªõng k·∫øt lu·∫≠n")
        elif confidence < 0.6:
            reasoning.append("‚Ä¢ **T√≠n hi·ªáu m√¢u thu·∫´n** - m·ªôt s·ªë feature kh√¥ng nh·∫•t qu√°n")
        
        # Specific decision drivers
        material_score = analysis.get('material_uniformity', 0.5) * analysis.get('surface_smoothness', 0.5)
        logo_score = analysis.get('logo_sharpness', 0.5)
        
        if material_score > 0.6 and logo_score > 0.6:
            reasoning.append("‚Ä¢ **Ch·∫•t l∆∞·ª£ng material + logo t·ªët** ‚Üí h·ªó tr·ª£ authentic")
        elif material_score < 0.4 or logo_score < 0.3:
            reasoning.append("‚Ä¢ **Ch·∫•t l∆∞·ª£ng material ho·∫∑c logo k√©m** ‚Üí nghi ng·ªù fake")
        
        return "\n".join(reasoning)

    def _break_down_confidence(self, analysis: Dict, confidence: float) -> str:
        """Break down confidence into contributing factors"""
        factors = []
        
        # Material quality contribution
        material_uniformity = analysis.get('material_uniformity', 0.5)
        if material_uniformity > 0.8:
            factors.append("‚Ä¢ **Ch·∫•t li·ªáu ƒë·ªìng ƒë·ªÅu** (+15% confidence)")
        elif material_uniformity < 0.4:
            factors.append("‚Ä¢ **Ch·∫•t li·ªáu kh√¥ng ƒë·ªÅu** (-10% confidence)")
        
        # Logo/text quality
        logo_sharpness = analysis.get('logo_sharpness', 0.5)
        if logo_sharpness > 0.7:
            factors.append("‚Ä¢ **Logo s·∫Øc n√©t** (+10% confidence)")
        elif logo_sharpness < 0.3:
            factors.append("‚Ä¢ **Logo m·ªù** (-15% confidence)")
        
        # Color vibrancy
        color_vibrancy = analysis.get('color_vibrancy', 0.5)
        if color_vibrancy > 0.7:
            factors.append("‚Ä¢ **M√†u s·∫Øc t·ª± nhi√™n** (+5% confidence)")
        elif color_vibrancy < 0.3:
            factors.append("‚Ä¢ **M√†u s·∫Øc nh·∫°t** (-5% confidence)")
        
        # Attention pattern
        concentration = analysis.get('attention_concentration', 0.5)
        if concentration > 0.7:
            factors.append("‚Ä¢ **Focus r√µ r√†ng** (+8% confidence)")
        elif concentration < 0.3:
            factors.append("‚Ä¢ **Kh√¥ng focus** (-8% confidence)")
        
        # Surface texture
        texture_strength = analysis.get('texture_strength', 0.5)
        if texture_strength > 0.8:
            factors.append("‚Ä¢ **Texture chi ti·∫øt** (+7% confidence)")
        elif texture_strength < 0.2:
            factors.append("‚Ä¢ **Texture thi·∫øu** (-10% confidence)")
        
        if confidence > 0.8:
            factors.append(f"\n**K·∫øt lu·∫≠n**: Nhi·ªÅu y·∫øu t·ªë t√≠ch c·ª±c ‚Üí **{confidence:.1%} confidence**")
        elif confidence < 0.6:
            factors.append(f"\n**K·∫øt lu·∫≠n**: C√°c y·∫øu t·ªë m√¢u thu·∫´n ‚Üí **{confidence:.1%} confidence**")
        else:
            factors.append(f"\n**K·∫øt lu·∫≠n**: C√¢n b·∫±ng gi·ªØa c√°c y·∫øu t·ªë ‚Üí **{confidence:.1%} confidence**")
        
        return "\n".join(factors)

    def visualize_explanation(self, original_image: np.ndarray, result: Dict, save_path: Optional[str] = None) -> plt.Figure:
        """üî• ENHANCED: Create comprehensive and beautiful visualization"""
        
        # Determine layout based on available heatmaps
        num_heatmaps = len(result.get('all_heatmaps', {'main': result['heatmap']}))
        
        if num_heatmaps <= 3:
            fig, axes = plt.subplots(2, 3, figsize=(24, 16))
        else:
            fig, axes = plt.subplots(3, 3, figsize=(24, 24))
            
        fig.suptitle(f"üî• ENHANCED AI Analysis: {result['prediction']} ({result['confidence']:.1%})", 
                     fontsize=24, weight='bold', color='darkblue')

        # Flatten axes for easier indexing
        axes_flat = axes.flatten()
        plot_idx = 0

        # 1. Original Image
        axes_flat[plot_idx].imshow(original_image)
        axes_flat[plot_idx].set_title('üì∏ Original Image', fontsize=16, weight='bold')
        axes_flat[plot_idx].axis('off')
        plot_idx += 1

        # 2. Primary Enhanced Heatmap
        primary_heatmap = result['heatmap']
        im1 = axes_flat[plot_idx].imshow(primary_heatmap, cmap=self.cmap_heat)
        axes_flat[plot_idx].set_title('üî• Enhanced Attention Heatmap', fontsize=16, weight='bold')
        axes_flat[plot_idx].axis('off')
        fig.colorbar(im1, ax=axes_flat[plot_idx], shrink=0.8, label='Attention Intensity')
        plot_idx += 1

        # 3. Attention Overlay
        try:
            overlay = original_image.astype(float) / 255.0
            heatmap_colored = self.cmap_heat(primary_heatmap)[:, :, :3]
            blended = 0.65 * overlay + 0.35 * heatmap_colored
            axes_flat[plot_idx].imshow(blended)
        except Exception as e:
            axes_flat[plot_idx].imshow(original_image)
            
        axes_flat[plot_idx].set_title('üéØ Attention Overlay', fontsize=16, weight='bold')
        axes_flat[plot_idx].axis('off')
        plot_idx += 1

        # 4-6. Multiple Heatmap Types (if available)
        all_heatmaps = result.get('all_heatmaps', {})
        heatmap_titles = {
            'gradient': 'üß† Gradient-Based Attention',
            'weights': '‚öñÔ∏è Attention Weights',
            'activation': 'üí´ Activation-Based',
            'fused': 'üîÄ Multi-Method Fusion',
            'enhanced': '‚ú® Enhanced Processing'
        }
        
        colormaps = [self.cmap_heat, self.cmap_cool, self.cmap_focus, 'viridis', 'plasma']
        
        for i, (hmap_type, hmap_data) in enumerate(all_heatmaps.items()):
            if plot_idx >= len(axes_flat) - 1 or hmap_type == 'enhanced':  # Skip enhanced as it's already shown
                continue
                
            if hmap_data is not None:
                cmap_to_use = colormaps[i % len(colormaps)]
                im = axes_flat[plot_idx].imshow(hmap_data, cmap=cmap_to_use)
                title = heatmap_titles.get(hmap_type, f'{hmap_type.title()} Attention')
                axes_flat[plot_idx].set_title(title, fontsize=14, weight='bold')
                axes_flat[plot_idx].axis('off')
                fig.colorbar(im, ax=axes_flat[plot_idx], shrink=0.6)
                plot_idx += 1

        # 7. Attention Statistics Visualization
        if plot_idx < len(axes_flat):
            self._plot_attention_statistics(axes_flat[plot_idx], result['content_analysis'])
            plot_idx += 1

        # 8. Focus Region Analysis
        if plot_idx < len(axes_flat):
            self._plot_focus_regions(axes_flat[plot_idx], original_image, primary_heatmap)
            plot_idx += 1

        # Hide remaining empty subplots
        for i in range(plot_idx, len(axes_flat)):
            axes_flat[i].axis('off')

        # Add comprehensive explanation text
        explanation_text = result['explanation']
        # Truncate if too long for display
        if len(explanation_text) > 800:
            explanation_text = explanation_text[:800] + "..."
            
        plt.figtext(0.5, 0.02, explanation_text, ha="center", fontsize=11, 
                   wrap=True, bbox={"facecolor": "lightblue", "alpha": 0.7, "pad": 10})
        
        plt.tight_layout(rect=[0, 0.15, 1, 0.95])

        if save_path:
            ensure_dir(os.path.dirname(save_path))
            plt.savefig(save_path, dpi=200, bbox_inches='tight', facecolor='white')
            print(f"üíæ Enhanced visualization saved to {save_path}")

        return fig

    def _plot_attention_statistics(self, ax, analysis: Dict):
        """Plot attention pattern statistics"""
        stats_to_plot = []
        values = []
        
        if 'attention_concentration' in analysis:
            stats_to_plot.extend(['Concentration', 'Symmetry', 'Dispersion'])
            values.extend([
                analysis['attention_concentration'],
                analysis['attention_symmetry'], 
                analysis['attention_dispersion']
            ])
        
        if 'high_attention_area' in analysis:
            stats_to_plot.extend(['High Focus %', 'Medium Focus %'])
            values.extend([
                analysis['high_attention_area'] / 100,
                analysis['medium_attention_area'] / 100
            ])
        
        if stats_to_plot and values:
            colors = plt.cm.Set3(np.linspace(0, 1, len(stats_to_plot)))
            bars = ax.bar(stats_to_plot, values, color=colors)
            ax.set_title('üìä Attention Pattern Statistics', fontsize=14, weight='bold')
            ax.set_ylim(0, 1)
            ax.set_ylabel('Value')
            
            # Add value labels on bars
            for bar, value in zip(bars, values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{value:.2f}', ha='center', va='bottom', fontsize=10)
        else:
            ax.text(0.5, 0.5, 'No Statistics\nAvailable', ha='center', va='center',
                   transform=ax.transAxes, fontsize=14)
            ax.set_title('üìä Attention Statistics', fontsize=14, weight='bold')

    def _plot_focus_regions(self, ax, image: np.ndarray, heatmap: np.ndarray):
        """Visualize focus regions with contours"""
        try:
            # Create focus contours
            high_thresh = np.percentile(heatmap, 90)
            medium_thresh = np.percentile(heatmap, 70)
            
            high_mask = (heatmap > high_thresh).astype(np.uint8)
            medium_mask = (heatmap > medium_thresh).astype(np.uint8)
            
            # Show original image
            ax.imshow(image)
            
            # Add contours
            if np.any(high_mask):
                high_contours, _ = cv2.findContours(high_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                for contour in high_contours:
                    contour = contour.squeeze()
                    if len(contour.shape) == 2 and contour.shape[0] > 2:
                        ax.plot(contour[:, 0], contour[:, 1], 'r-', linewidth=3, label='High Focus')
            
            if np.any(medium_mask):
                medium_contours, _ = cv2.findContours(medium_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                for contour in medium_contours:
                    contour = contour.squeeze()
                    if len(contour.shape) == 2 and contour.shape[0] > 2:
                        ax.plot(contour[:, 0], contour[:, 1], 'y--', linewidth=2, label='Medium Focus')
            
            ax.set_title('üéØ Focus Region Analysis', fontsize=14, weight='bold')
            ax.axis('off')
            
            # Add legend
            handles, labels = ax.get_legend_handles_labels()
            if handles:
                ax.legend(handles[:2], labels[:2], loc='upper right')  # Avoid duplicate labels
                
        except Exception as e:
            ax.text(0.5, 0.5, f'Focus Analysis\nError: {str(e)[:30]}...', 
                   ha='center', va='center', transform=ax.transAxes, fontsize=12)
            ax.set_title('üéØ Focus Region Analysis', fontsize=14, weight='bold')

def ensure_dir(directory: str):
    if directory and not os.path.exists(directory):
        os.makedirs(directory)


def generate_image_metrics(image: np.ndarray) -> Dict:
    """Ph√¢n t√≠ch c√°c ch·ªâ s·ªë h√¨nh ·∫£nh c∆° b·∫£n ph·ª•c v·ª• gi·∫£i th√≠ch"""
    from scipy import ndimage

    gray = np.mean(image, axis=2).astype(np.float32)

    # M·ª©c ƒë·ªô s·∫Øc n√©t (Sharpness)
    sobel_x = ndimage.sobel(gray, axis=0)
    sobel_y = ndimage.sobel(gray, axis=1)
    sharpness = np.mean(np.sqrt(sobel_x**2 + sobel_y**2)) / 255.0

    # ƒê·ªô ƒë·ªëi x·ª©ng (Symmetry) theo chi·ªÅu ngang
    flipped = np.fliplr(gray)
    symmetry = 1.0 - np.mean(np.abs(gray - flipped)) / 255.0

    # M·ª©c ƒë·ªô chi ti·∫øt (Texture)
    laplacian = ndimage.laplace(gray)
    texture = np.std(laplacian) / 255.0

    # T∆∞∆°ng ph·∫£n vi·ªÅn (Edge precision)
    edge = ndimage.sobel(gray)
    edge_precision = np.mean(np.abs(edge)) / 255.0

    return {
        "sharpness": round(sharpness, 3),
        "symmetry": round(symmetry, 3),
        "texture": round(texture, 3),
        "edge_precision": round(edge_precision, 3)
    }


def summarize_verdict(confidence: float) -> str:
    """T√≥m t·∫Øt k·∫øt lu·∫≠n cu·ªëi"""
    if confidence > 0.9:
        return "‚úîÔ∏è C√≥ kh·∫£ nƒÉng r·∫•t cao ƒë√¢y l√† h√†ng CH√çNH H√ÉNG"
    elif confidence > 0.7:
        return "‚ö†Ô∏è C√≥ v·∫ª l√† ch√≠nh h√£ng, nh∆∞ng c·∫ßn ki·ªÉm tra th√™m"
    else:
        return "‚ùå C√≥ kh·∫£ nƒÉng l√† h√†ng FAKE ho·∫∑c kh√¥ng ƒë·ªß c∆° s·ªü k·∫øt lu·∫≠n"


def generate_ai_analysis(metrics: Dict, confidence: float) -> str:
    """Sinh l·ªùi ph√¢n t√≠ch d·ª±a tr√™n ch·ªâ s·ªë th·∫≠t"""
    text = []
    text.append("üîç **ƒê√ÅNH GI√Å AI D·ª∞A TR√äN H√åNH ·∫¢NH**")

    sharpness = metrics["sharpness"]
    symmetry = metrics["symmetry"]
    texture = metrics["texture"]
    edge = metrics["edge_precision"]

    # S·∫Øc n√©t
    if sharpness > 0.6:
        text.append("- H√¨nh ·∫£nh c√≥ ƒë·ªô s·∫Øc n√©t cao, chi ti·∫øt hi·ªÉn th·ªã r√µ r√†ng.")
    elif sharpness > 0.4:
        text.append("- M·ª©c ƒë·ªô s·∫Øc n√©t t∆∞∆°ng ƒë·ªëi ·ªïn ƒë·ªãnh.")
    else:
        text.append("- H√¨nh ·∫£nh kh√° m·ªù, thi·∫øu chi ti·∫øt n·ªïi b·∫≠t.")

    # Texture
    if texture > 0.08:
        text.append("- B·ªÅ m·∫∑t s·∫£n ph·∫©m c√≥ ƒë·ªô texture ph·ª©c t·∫°p, gi·ªëng ƒë·∫∑c tr∆∞ng h√†ng th·∫≠t.")
    elif texture > 0.04:
        text.append("- Texture ·ªü m·ª©c trung b√¨nh, kh√≥ ph√¢n bi·ªát r√µ.")
    else:
        text.append("- B·ªÅ m·∫∑t m·ªãn, texture ƒë∆°n gi·∫£n ‚Äî d·∫•u hi·ªáu c·ªßa h√†ng nh√°i.")

    # Symmetry
    if symmetry > 0.85:
        text.append("- S·∫£n ph·∫©m ƒë·ªëi x·ª©ng cao, cho th·∫•y thi·∫øt k·∫ø chu·∫©n x√°c.")
    elif symmetry > 0.7:
        text.append("- C√≥ m·ªôt s·ªë sai l·ªách ƒë·ªëi x·ª©ng nh·ªè.")
    else:
        text.append("- ƒê·ªëi x·ª©ng k√©m ‚Äî kh·∫£ nƒÉng cao l√† l·ªói gia c√¥ng ho·∫∑c b·∫£n copy.")

    # Edge precision
    if edge > 0.5:
        text.append("- C√°c ƒë∆∞·ªùng n√©t r√µ r√†ng, vi·ªÅn s·∫Øc ‚Äî ƒëi·ªÉm c·ªông cho h√†ng chu·∫©n.")
    elif edge > 0.3:
        text.append("- Vi·ªÅn h∆°i m·ªÅm, chi ti·∫øt ch∆∞a r√µ n√©t.")
    else:
        text.append("- Vi·ªÅn m·ªù, chi ti·∫øt kh√¥ng r√µ ‚Äî c·∫ßn ki·ªÉm tra k·ªπ h∆°n.")

    # K·∫øt lu·∫≠n cu·ªëi
    text.append(f"\nüß† **K·∫øt lu·∫≠n AI**: {summarize_verdict(confidence)}")

    return "\n".join(text)


def generate_heatmap(original_img: np.ndarray, cam: np.ndarray, save_path: str = "results/heatmap.jpg") -> str:
    """
    √Åp d·ª•ng Grad-CAM (cam) l√™n ·∫£nh g·ªëc v√† l∆∞u heatmap k·∫øt qu·∫£.
    - original_img: ·∫£nh RGB d·∫°ng ndarray
    - cam: m·∫£ng heatmap (gi√° tr·ªã t·ª´ 0‚Äì1), th∆∞·ªùng l√† output t·ª´ Grad-CAM
    - save_path: n∆°i l∆∞u heatmap.jpg
    """
    # Resize CAM v·ªÅ ƒë√∫ng k√≠ch th∆∞·ªõc ·∫£nh g·ªëc
    cam_resized = cv2.resize(cam, (original_img.shape[1], original_img.shape[0]))
    cam_resized = np.uint8(255 * cam_resized)

    # Apply heatmap m√†u
    heatmap = cv2.applyColorMap(cam_resized, cv2.COLORMAP_JET)

    # Convert ·∫£nh g·ªëc sang BGR ƒë·ªÉ blend ƒë√∫ng (v√¨ OpenCV d√πng BGR)
    original_bgr = cv2.cvtColor(original_img, cv2.COLOR_RGB2BGR)

    # Alpha blend gi·ªØa ·∫£nh g·ªëc v√† heatmap
    blended = cv2.addWeighted(original_bgr, 0.6, heatmap, 0.4, 0)

    # L∆∞u heatmap
    cv2.imwrite(save_path, blended)

    return save_path